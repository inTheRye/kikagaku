{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# キカガク「自然言語処理の基礎（PyTorch）」の学習メモ\n",
    "* https://www.kikagaku.ai/tutorial/basic_of_nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 導入\n",
    "* 基礎技術\n",
    "    * 形態素解析\n",
    "        * 文章を単語ごとに切り分ける処理\n",
    "    * 構文解析\n",
    "        * 単語の係り受けなど、就職関係を決める処理\n",
    "    * 意味解析\n",
    "        * 「意味が妥当であるか」など、文章の意味を決定する処理\n",
    "        * 例：望遠鏡で泳ぐ彼女を見た\n",
    "    * 文脈解析\n",
    "        * 複数の文について関係を決定する処理\n",
    "        * 例：入り口に男性が立っている。彼の名前は佐川だ\n",
    "\n",
    "* 自然言語処理の流れ\n",
    "    * データの収集\n",
    "        * 解決したいタスクに応じてデータを収集する\n",
    "    * クリーニング処理→ここは実質スクレイピング\n",
    "        * HTMLのタグなど意味を持たないノイズを削除する\n",
    "    * 単語の正規化\n",
    "        * 半角や全角、小文字大文字などを統一する\n",
    "    * 形態素解析(単語分割)\n",
    "        * 文章を単語ごと分割する\n",
    "    * 基本形への変換\n",
    "        * 語幹(活用しない部分)への統一を行う\n",
    "        * 例：学ん(だ)→学ぶ\n",
    "        * 昨今の実装では基本形へ変換しない場合もある\n",
    "    * ストップワード除去\n",
    "        * 出現回数の多すぎる単語など、役に立たない単語を除去する\n",
    "        * 昨今の実装では除去しない場合もある\n",
    "    * 単語の数値化\n",
    "        * 機械学習で扱えるよう文字列から数値へ変換を行う\n",
    "    * モデルの学習\n",
    "        * タスクに合わせ、古典的な機械学習~ニューラルネットワーク選択する\n",
    "\n",
    "* 代表的なデータセット\n",
    "    * 日本政府や自治体\n",
    "        * https://www.data.go.jp/\n",
    "    * 教育機関\n",
    "        * 東北大学 乾・鈴木研究室 Open Resources  \n",
    "        * 首都大学東京 自然言語処理研究室（小町研）  \n",
    "        * 京都大学 黒橋・河原・村脇研究室 自然言語処理のためのリソース\n",
    "        * Graham Neubig 日本語対訳データ\n",
    "    * 民間、企業\n",
    "        * 青空文庫\n",
    "        * livedoorニュースコーパス\n",
    "        * Amazonレビューデータセット\n",
    "    * Kaggle Dataset\n",
    "        * https://www.kaggle.com/datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ハンズオン\n",
    "### 先ずはMeCab等の基本的な使い方から"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "mecab = MeCab.Tagger('-Ochasen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = mecab.parse('こんにちは、私はキカガクです。')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "こんにちは\tコンニチハ\tこんにちは\t感動詞\t\t\n",
      "、\t、\t、\t記号-読点\t\t\n",
      "私\tワタシ\t私\t名詞-代名詞-一般\t\t\n",
      "は\tハ\tは\t助詞-係助詞\t\t\n",
      "キカガク\tキカガク\tキカガク\t名詞-一般\t\t\n",
      "です\tデス\tです\t助動詞\t特殊・デス\t基本形\n",
      "。\t。\t。\t記号-句点\t\t\n",
      "EOS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = 'キカガクでは、ディープラーニングを含んだ機械学習や人工知能の教育を行っています。'\n",
    "text2 = '代表の吉崎は大学院では機械学習・ロボットのシステム制御、画像処理の研究に携わっていました。'\n",
    "text3 = '機械学習、システム制御、画像処理ではすべて線形代数とプログラミングが不可欠になります。'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "キカガク\tキカガク\tキカガク\t名詞-一般\t\t\n",
      "で\tデ\tで\t助詞-格助詞-一般\t\t\n",
      "は\tハ\tは\t助詞-係助詞\t\t\n",
      "、\t、\t、\t記号-読点\t\t\n",
      "ディープラーニング\tディープラーニング\tディープラーニング\t名詞-一般\t\t\n",
      "を\tヲ\tを\t助詞-格助詞-一般\t\t\n",
      "含ん\tフクン\t含む\t動詞-自立\t五段・マ行\t連用タ接続\n",
      "だ\tダ\tだ\t助動詞\t特殊・タ\t基本形\n",
      "機械\tキカイ\t機械\t名詞-一般\t\t\n",
      "学習\tガクシュウ\t学習\t名詞-サ変接続\t\t\n",
      "や\tヤ\tや\t助詞-並立助詞\t\t\n",
      "人工\tジンコウ\t人工\t名詞-一般\t\t\n",
      "知能\tチノウ\t知能\t名詞-一般\t\t\n",
      "の\tノ\tの\t助詞-連体化\t\t\n",
      "教育\tキョウイク\t教育\t名詞-サ変接続\t\t\n",
      "を\tヲ\tを\t助詞-格助詞-一般\t\t\n",
      "行っ\tオコナッ\t行う\t動詞-自立\t五段・ワ行促音便\t連用タ接続\n",
      "て\tテ\tて\t助詞-接続助詞\t\t\n",
      "い\tイ\tいる\t動詞-非自立\t一段\t連用形\n",
      "ます\tマス\tます\t助動詞\t特殊・マス\t基本形\n",
      "。\t。\t。\t記号-句点\t\t\n",
      "EOS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 形態素解析\n",
    "res = mecab.parse(text1)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns = [] # 品詞が名詞 (noun) である単語を格納するリスト\n",
    "res = mecab.parse(text1)\n",
    "words = res.split('\\n')[:-2]\n",
    "for word in words:\n",
    "    part = word.split('\\t')\n",
    "    if '名詞' in part[3]:\n",
    "        nouns.append(part[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['キカガク', 'ディープラーニング', '機械', '学習', '人工', '知能', '教育']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 名刺抽出処理を関数化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nouns(text):\n",
    "    nouns = []\n",
    "    res = mecab.parse(text)\n",
    "    words = res.split('\\n')[:-2]\n",
    "    for word in words:\n",
    "        part = word.split('\\t')\n",
    "        if '名詞' in part[3]:\n",
    "            nouns.append(part[0])\n",
    "    return nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基礎的なテキストデータのエンコーディング方法（ベクトル化）\n",
    "\n",
    "* Bag of Words (Count encoding)\n",
    "* tf-idf\n",
    "* One-hot encoding\n",
    "* Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [text1, text2, text3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns_list = [ get_nouns(i) for i in texts ]\n",
    "corpus = []\n",
    "for nouns in nouns_list:\n",
    "  corpus.append(' '.join(nouns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['キカガク ディープラーニング 機械 学習 人工 知能 教育',\n",
       " '代表 吉崎 大学院 機械 学習 ロボット システム 制御 画像 処理 研究',\n",
       " '機械 学習 システム 制御 画像 処理 すべて 線形 代数 プログラミング 不可欠']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of wordsのベクトルを作る\n",
    "* fit_transform() メソッドを使用すると、前述の説明通り、単語毎に ID が割り振られ、ID ごとの出現回数を元にベクトル化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "x = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* vocabulary_ 属性でコーパス中のエンコーディングされた単語とその ID を確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'キカガク': 1,\n",
       " 'ディープラーニング': 3,\n",
       " '機械': 16,\n",
       " '学習': 14,\n",
       " '人工': 7,\n",
       " '知能': 18,\n",
       " '教育': 15,\n",
       " '代表': 9,\n",
       " '吉崎': 12,\n",
       " '大学院': 13,\n",
       " 'ロボット': 5,\n",
       " 'システム': 2,\n",
       " '制御': 11,\n",
       " '画像': 17,\n",
       " '処理': 10,\n",
       " '研究': 19,\n",
       " 'すべて': 0,\n",
       " '線形': 20,\n",
       " '代数': 8,\n",
       " 'プログラミング': 4,\n",
       " '不可欠': 6}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### エンコーディング後の数値を toarray() メソッドを使用して取得\n",
    "\n",
    "* それぞれが、text1、text2、text3に対応したベクトル。\n",
    "* それぞれのリストの値は出現数。リストのindexは単語のIDに対応。\n",
    "* text1は`キカガク ディープラーニング 機械 学習 人工 知能 教育`で、IDに直すと`1, 3, 16, 14, 7, 18, 15`なので、リストの該当インデックスが1となっているのがわかる。\n",
    "  \n",
    "* [キカガクのサイト](https://www.kikagaku.ai/tutorial/basic_of_nlp/learn/pytorch_text_classification)だと下記のように書いてあるが、コーパスのIDと合っていないので間違い。（人工知能と機械学習はコーパスに存在しない。→区切りが異なる。）\n",
    "```\n",
    " 'キカガク': 1,\n",
    " 'ディープラーニング': 3,\n",
    " '人工知能': 7,\n",
    " '教育': 12,\n",
    " '機械学習': 13, \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0],\n",
       "       [1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データを取ってきて、'content/text'配下に配置。\n",
    "* データセットは[こちら](http://drive.google.com/uc?export=view&id=1J4N95vciIIe5w18eXpC2KYvL0DzUoITd)からダウンロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['content/text/movie-enter',\n",
       " 'content/text/it-life-hack',\n",
       " 'content/text/kaden-channel',\n",
       " 'content/text/topic-news',\n",
       " 'content/text/livedoor-homme',\n",
       " 'content/text/peachy',\n",
       " 'content/text/sports-watch',\n",
       " 'content/text/smax']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob\n",
    "\n",
    "# ディレクトリの取得\n",
    "directories = glob('content/text/*')\n",
    "directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['content/text/movie-enter/movie-enter-5978741.txt',\n",
       " 'content/text/movie-enter/movie-enter-6322901.txt',\n",
       " 'content/text/movie-enter/movie-enter-6176324.txt']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepaths = glob('{}/*.txt'.format(directories[0]))\n",
    "filepaths[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 試しに１つファイルを読み込んで中身を見てみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【DVDエンター！】誘拐犯に育てられた女が目にした真実は、孤独か幸福か\n",
      "　2005年11月から翌2006年7月まで読売新聞にて連載された、直木賞作家・角田光代による初の長編サスペンス『八日目の蝉』。2010年に檀れいと北乃きいの出演によりテレビドラマ化された同作が、2011年4月に永作博美と井上真央の出演によって映画化。そして、劇場公開から半年が過ぎた10月28日、DVD＆ブルーレイとなって発売されました。\n",
      "\n",
      "八日目の蝉\n",
      "　妻子ある男と愛し合い、その子を身ごもりながら、あきらめざるをえなかった女。彼女は同時に、男の妻が子供を産んだことを知る。その赤ん坊を見に行った女は、突発的にその子を連れ去り、逃避行を続けた挙句、小豆島に落ち着き、母と娘として暮らしはじめる。\n",
      "\n",
      "\n",
      "不倫相手の子供を誘拐し、4年間育てた女\n",
      "　永作博美が演じる野々宮希和子は、不倫相手の子を宿しながらも、彼の「いずれ妻と別れるから、それまで待ってくれ」という常套句を信じて、中絶。後遺症により、二度と子供を産めない身体となってしまいます。その後、不倫相手から彼の妻が出産したことを知らされ、別れを決意。最後に諦めをつけるため、彼らの生後6ヶ月の赤ん坊・恵理菜の顔を見た希和子でしたが、自分に笑顔で向けた恵理菜を見て、思わず誘拐。名前を変えて恵理菜を薫と名付けると、人目を避けて各地を転々とし、二人で幸せな時間を過ごしますが、辿り着いた最後の場所・小豆島で4年の逃避行に終止符を打ちます。\n",
      "\n",
      "\n",
      "誘拐犯に育てられた女\n",
      "　4歳になって実の両親と再会を果たした後も、世間から言われの無い中傷を受け、本当の両親との関係を築けないまま、21歳の大学生へと成長した恵理菜。過去と向き合うことを避けてきた恵理菜でしたが、劇団ひとりが演じる不倫相手・岸田孝史の子を宿し、ずっと憎み続けてきた希和子と同じ道を歩んでいることに気付いた彼女は、小池栄子が演じるルポライター・安藤千草と共に、4年間の逃亡生活を追憶する旅に出ます。希和子との幸せだった時間に触れながら、最終地・小豆島に辿り着いた恵理菜が見た真実とは？\n",
      "\n",
      "\n",
      "八日目の蝉は幸せなのだろうか？\n",
      "　蝉は俗説として、一生の大半を幼虫として地下で費やし、地上に出て羽化からわずか1週間程度で死ぬと言われています。八日目まで生き残ってしまった蝉が目にしたのは、孤独か、あるいは誰も目にすることのできなかった世界なのでしょうか。中絶によって二度と子供を埋めない身体になった希和子が、恵理菜と過ごした4年間の“八日目”は、希和子だけでなく、恵理菜にとっても幸せな時間だったのではないでしょうか。\n",
      "\n",
      "　主題歌は、デビュー10年目に突入した2010年10月より、耳管開放症のため音楽活動を休止していた中島美嘉の復帰第一弾（通算33作目）シングル『Dear』。原作に深い感銘を受けた彼女が、本作のために提供した楽曲となっています。\n",
      "\n",
      "・八日目の蝉 - 作品情報\n",
      "\n",
      "八日目の蝉　通常版 [DVD]posted with amazlet at 11.10.29アミューズソフトエンタテインメント (2011-10-28)\n",
      "売り上げランキング: 266\n",
      "Amazon.co.jp で詳細を見る\n",
      "■関連記事\n",
      "・井上真央が逃避行？永作、森口、小池も頭を悩ます映画とは\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(filepaths[0], encoding='utf-8') as f:\n",
    "  text = ''.join(f.readlines()[2:])\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 全記事のテキストを読み込んで、１記事単位のリストにする。同じ順番でラベル（どのソースか）のIDも振る。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, labels = [], []\n",
    "for (i, directory) in enumerate(directories):\n",
    "    #各ディレクトリ内のtxtファイルのパスをすべて取得\n",
    "    filepaths = glob('{}/*.txt'.format(directory))\n",
    "    # テキストを読み込んで、内容をtextに格納、ラベルも併せて格納\n",
    "    for filepath in filepaths:\n",
    "        with open(filepath, encoding='utf-8') as f:\n",
    "            text = ''.join(f.readlines()[2:])  # URL等の先頭２行を除いた各行の文章を連結（join）して格納\n",
    "            texts.append(text)\n",
    "            labels.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6505, 6505)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts), len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 各テキストから名刺を抽出して、空白区切りにしてword_colect(List)に格納"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_collect = []\n",
    "for text in texts:\n",
    "    nouns = get_nouns(text)\n",
    "    word_collect.append(' '.join(nouns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BoW に変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df=20)\n",
    "x = vectorizer.fit_transform(word_collect)\n",
    "x = x.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorchで取り扱えるデータ形式に変換\n",
    "* BoWベクトル：numpy.ndarrayからtorch.tensorへ\n",
    "* labels：listからtorch.tensorへ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x = torch.tensor(x, dtype=torch.float32)\n",
    "t = torch.tensor(labels, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pickle化する関数を定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data\n",
    "import pickle\n",
    "\n",
    "def to_pickle(obj, file):\n",
    "    with open(file, 'wb') as f:\n",
    "        pickle.dump(obj, f, protocol=4)\n",
    "        \n",
    "def from_pickle(file):\n",
    "    with open(file, 'rb') as f:\n",
    "        obj = pickle.load(f)\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bowのデータとラベルデータをまとめたデータセットを作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.TensorDataset at 0x1270ac890>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = torch.utils.data.TensorDataset(x, t)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習用データセットとテスト用データセットの分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = int(len(dataset) * 0.6)\n",
    "n_val = int(len(dataset) * 0.2)\n",
    "n_test = len(dataset) - n_train - n_val\n",
    "\n",
    "      \n",
    "# データの分割\n",
    "torch.manual_seed(0)\n",
    "\n",
    "train, val, test = torch.utils.data.random_split(dataset, [n_train, n_val, n_test])\n",
    "\n",
    "to_pickle(train, 'data/train.pickle')\n",
    "to_pickle(val,   'data/val.pickle')\n",
    "to_pickle(test,  'data/test.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルの構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainNet(pl.LightningModule):\n",
    "\n",
    "    @pl.data_loader\n",
    "    def train_dataloader(self):\n",
    "        #train = from_pickle('data/train.pickle')\n",
    "        return torch.utils.data.DataLoader(train, self.batch_size, shuffle=True, num_workers=self.num_workers)\n",
    "\n",
    "    def training_step(self, batch, batch_nb):\n",
    "        x, t = batch\n",
    "        y = self.forward(x)\n",
    "        loss = self.lossfun(y, t)\n",
    "        results = {'loss': loss}\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidationNet(pl.LightningModule):\n",
    "\n",
    "    @pl.data_loader\n",
    "    def val_dataloader(self):\n",
    "        #val = from_pickle('data/val.pickle')\n",
    "        return torch.utils.data.DataLoader(val, self.batch_size, num_workers=self.num_workers)\n",
    "\n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        x, t = batch\n",
    "        y = self.forward(x)\n",
    "        loss = self.lossfun(y, t)\n",
    "        y_label = torch.argmax(y, dim=1)\n",
    "        acc = torch.sum(t == y_label) * 1.0 / len(t)\n",
    "        results = {'val_loss': loss, 'val_acc': acc}\n",
    "        return results\n",
    "\n",
    "    def validation_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        avg_acc = torch.stack([x['val_acc'] for x in outputs]).mean()\n",
    "        results = {'val_loss': avg_loss, 'val_acc': avg_acc}\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestNet(pl.LightningModule):\n",
    "\n",
    "    @pl.data_loader\n",
    "    def test_dataloader(self):\n",
    "        #test = from_pickle('data/test.pickle')\n",
    "        return torch.utils.data.DataLoader(test, self.batch_size, num_workers=self.num_workers)\n",
    "\n",
    "    def test_step(self, batch, batch_nb):\n",
    "        x, t = batch\n",
    "        y = self.forward(x)\n",
    "        loss = self.lossfun(y, t)\n",
    "        y_label = torch.argmax(y, dim=1)\n",
    "        acc = torch.sum(t == y_label) * 1.0 / len(t)\n",
    "        results = {'test_loss': loss, 'test_acc': acc}\n",
    "        return results\n",
    "\n",
    "    def test_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n",
    "        avg_acc = torch.stack([x['test_acc'] for x in outputs]).mean()\n",
    "        results = {'test_loss': avg_loss, 'test_acc': avg_acc}\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(TrainNet, ValidationNet, TestNet):\n",
    "\n",
    "    def __init__(self, batch_size=128, num_workers=8):\n",
    "        super(Net, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.fc1 = nn.Linear(5795, 200)\n",
    "        self.fc2 = nn.Linear(200, 9)\n",
    "\n",
    "    def lossfun(self, y, t):\n",
    "        return F.cross_entropy(y, t)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cuDNN に対する再現性の確保\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c69ac3c1a35d4702885bae8cc1fe9319",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Validation sanity check', layout=Layout(flex='2'), max=5.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d731cfe67128497a839e0654dd3d7090",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max=1.0), HTML(value='')), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33646d6b8f6c4ff89093722cecfe714b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Validating', layout=Layout(flex='2'), max=11.0, style=Pro…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6582d5c0c4984b20a12e26a845184024",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Validating', layout=Layout(flex='2'), max=11.0, style=Pro…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a8733b0ed1048338f13eac82532646c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Validating', layout=Layout(flex='2'), max=11.0, style=Pro…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df35e04f44c34c3ca5f6051250a98909",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Validating', layout=Layout(flex='2'), max=11.0, style=Pro…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7deb8ee360a84dca9f104a2b56af19b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Validating', layout=Layout(flex='2'), max=11.0, style=Pro…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa7e8eb117f44461b3776996ee91ee28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Validating', layout=Layout(flex='2'), max=11.0, style=Pro…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd56958e27ca416db50b11789923baf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Validating', layout=Layout(flex='2'), max=11.0, style=Pro…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c52409b9dbcd49aba3968a381e3b31fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Validating', layout=Layout(flex='2'), max=11.0, style=Pro…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92f2988950fe4d588b1fe9f700a5b52b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Validating', layout=Layout(flex='2'), max=11.0, style=Pro…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acd4913139c34459b347140ed275bfc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Validating', layout=Layout(flex='2'), max=11.0, style=Pro…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d34e4bf30874ab0b13b63e094b15771",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Validating', layout=Layout(flex='2'), max=11.0, style=Pro…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d2b5c0e6f854df0b2f36f33914c56f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Validating', layout=Layout(flex='2'), max=11.0, style=Pro…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ed17dd449b544b38126422eb0cf2a4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Validating', layout=Layout(flex='2'), max=11.0, style=Pro…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "004d2df29dcb49388049cb3e46e8f3ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Validating', layout=Layout(flex='2'), max=11.0, style=Pro…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bf1e3056df94ca4ba8591458f55bd2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Validating', layout=Layout(flex='2'), max=11.0, style=Pro…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67d59701607343db8d8eeb7be51e5274",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Validating', layout=Layout(flex='2'), max=11.0, style=Pro…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00757a7ad1a24619aaaf478a72c6cd68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Validating', layout=Layout(flex='2'), max=11.0, style=Pro…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fbb0806ebcd4b16b1c5183f43ccceb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Validating', layout=Layout(flex='2'), max=11.0, style=Pro…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64801b30fb9140fda240cb29a64a3172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Validating', layout=Layout(flex='2'), max=11.0, style=Pro…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "702eaba4bc8a4328bbd73ce4e868fdd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Validating', layout=Layout(flex='2'), max=11.0, style=Pro…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 乱数のシードを固定\n",
    "torch.manual_seed(0)\n",
    "\n",
    "net = Net(batch_size=128)\n",
    "trainer = Trainer(max_nb_epochs=20)\n",
    "\n",
    "# モデルの学習\n",
    "trainer.fit(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': 3.9073142943379935e-06,\n",
       " 'val_loss': 0.4451643228530884,\n",
       " 'val_acc': 0.9452448487281799,\n",
       " 'epoch': 19}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.callback_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f946a5b2198490f96edca601aca938d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Testing', layout=Layout(flex='2'), max=11.0, style=Progre…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "TEST RESULTS\n",
      "{}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 3.9073142943379935e-06,\n",
       " 'val_loss': 0.4451643228530884,\n",
       " 'val_acc': 0.9452448487281799,\n",
       " 'epoch': 19,\n",
       " 'test_loss': 0.30323073267936707,\n",
       " 'test_acc': 0.9381425976753235}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# テストデータに対する検証\n",
    "trainer.test()\n",
    "\n",
    "# テストデータに対する結果\n",
    "trainer.callback_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
